{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunrui/anaconda3/envs/cell-tracking-challenge/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sunrui/anaconda3/envs/cell-tracking-challenge/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/sunrui/anaconda3/envs/cell-tracking-challenge/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZNK3c106IValue23reportToTensorTypeErrorEv\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "# cur_dir = os.path.split(os.path.abspath(__file__))[0]\n",
    "# con_pth = cur_dir.rsplit('/',2)[0]\n",
    "# sys.path.append(con_pth)\n",
    "from src.models.celltrack_plmodel import CellTrackLitModel\n",
    "from src.inference.graph_dataset_inference import CellTrackDataset\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/sunrui/cellwork/track/cell-tracker-gnn-main/logs/runs/2023-05-24/12-53-04/checkpoints/epoch=56.ckpt\"\n",
    "num_seq = \"01\"\n",
    "output_csv = '/data/sunrui/celldata/r03c06f04ch1/'\n",
    "\n",
    "path_output = output_csv\n",
    "second_path = num_seq\n",
    "path_output_folder = os.path.join(path_output, second_path) + '_RES_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = os.path.join(path_output_folder, 'pytorch_geometric_data.pt')\n",
    "file2 = os.path.join(path_output_folder, 'all_data_df.csv')\n",
    "file3 = os.path.join(path_output_folder, 'raw_output.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(file2)\n",
    "test_data = torch.load(file1)\n",
    "outputs = torch.load(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_connected_edges():\n",
    "    edge_index, df, outputs = edge_index, df_preds, output_pred\n",
    "\n",
    "    if not self.directed:\n",
    "        final_outputs_hard, edge_index = self.megre_match_edges(edge_index.detach().clone(), outputs.detach().clone())\n",
    "        self.outputs_hard = final_outputs_hard\n",
    "        self.edge_index = edge_index\n",
    "    else:\n",
    "        outputs_soft = torch.sigmoid(outputs)\n",
    "        self.outputs_hard = (outputs_soft > self.decision_threshold).int()\n",
    "# find_connected_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     0,     0,  ..., 16146, 16146, 16146],\n",
      "        [   53,    55,    74,  ..., 16154, 16164, 16171]])\n"
     ]
    }
   ],
   "source": [
    "print(test_data.edge_index)\n",
    "# i = 0\n",
    "# connected_indices = test_data.edge_index\n",
    "# ind_place = np.argwhere(connected_indices[0, :] == 0)\n",
    "# print(ind_place)\n",
    "# next_frame_ind = connected_indices[1, ind_place].numpy().squeeze()\n",
    "# print(next_frame_ind)\n",
    "# next_frame = df_preds.loc[next_frame_ind, [\"centroid_row\", \"centroid_col\"]].values\n",
    "# curr_node = df_preds.loc[i,[\"centroid_row\", \"centroid_col\"]].values\n",
    "# distance = ((next_frame - curr_node) ** 2).sum(axis=-1)\n",
    "# nearest_cell = np.argmin(distance, axis=-1)\n",
    "# # add to the array\n",
    "# next_node_ind = next_frame_ind[nearest_cell]\n",
    "# print(next_node_ind)\n",
    "\n",
    "# next_node_ind = connected_indices[1, ind_place[0]]\n",
    "# ind_place = ind_place\n",
    "# print(ind_place)\n",
    "# next_node_ind = connected_indices[1, ind_place[0]]\n",
    "# print(next_node_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 18.4344, -20.9180, -16.0451,  ...,  -3.7429, -11.5620,  17.1256])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(edge_index)\n",
    "# print(edge_index[:, ::2])\n",
    "# print(edge_index[[1, 0], 1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_data\n",
    "edge_index = data.edge_index\n",
    "df_preds = df_data\n",
    "output_pred = outputs\n",
    "decision_threshold = 0.5\n",
    "df = df_preds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0,  ..., 0, 0, 1], dtype=torch.int32)\n",
      "tensor([[    0,     1,     2,  ..., 16137, 16139, 16146],\n",
      "        [   53,    55,    54,  ..., 16167, 16154, 16171]])\n",
      "torch.Size([2, 20012])\n",
      "Connected Indices (After Filtering):\n",
      "tensor([[    0,     1,     2,  ..., 16137, 16139, 16146],\n",
      "        [   53,    55,    54,  ..., 16167, 16154, 16171]])\n",
      "torch.Size([2, 15373])\n"
     ]
    }
   ],
   "source": [
    "outputs_soft = torch.sigmoid(output_pred)\n",
    "outputs_hard = (outputs_soft > decision_threshold).int()\n",
    "print(outputs_hard)\n",
    "\n",
    "connected_indices = edge_index[:, outputs_hard.bool()]#取出符合要求的边缘\n",
    "output_save_pre = output_pred[outputs_hard.bool()]\n",
    "print(connected_indices)#change\n",
    "print(connected_indices.shape)\n",
    "\n",
    "label_fir = connected_indices[0,:]\n",
    "label_sec = connected_indices[1,:]\n",
    "label_sec_unique = np.unique(label_sec)\n",
    "\n",
    "\n",
    "# 获取每个标签的行和列\n",
    "first_positions = df.loc[label_fir, [\"centroid_row\", \"centroid_col\"]].values\n",
    "second_positions = df.loc[label_sec, [\"centroid_row\", \"centroid_col\"]].values\n",
    "\n",
    "# 计算距离矩阵\n",
    "distances = ((second_positions - first_positions) ** 2).sum(axis=-1)\n",
    "distances = np.sqrt(distances)\n",
    "\n",
    "# 创建一个矩阵来标识需要保留的连接\n",
    "keep_mask = np.zeros_like(distances, dtype=bool)\n",
    "\n",
    "\n",
    "for label in label_sec_unique:\n",
    "        label_indices = np.where(label_sec == label)[0]\n",
    "        if len(label_indices) > 1:\n",
    "            label_distances = distances[label_indices]\n",
    "            min_distance_index = np.argmin(label_distances)\n",
    "            keep_mask[label_indices[min_distance_index]] = True\n",
    "        else:\n",
    "            keep_mask[label_indices] = True\n",
    "        \n",
    "# 使用布尔索引删除不需要的连接\n",
    "connected_indices = connected_indices[:, keep_mask]\n",
    "output_save_pre = output_save_pre[keep_mask]\n",
    "\n",
    "# for label in label_sec_unique:\n",
    "#     label_cols = label_sec[label_sec == label]\n",
    "#     if label_cols.shape[0] > 1:\n",
    "      \n",
    "#         ind_place_col = np.argwhere(label_sec==label)\n",
    "      \n",
    "#         curr_position = df.loc[label,[\"centroid_row\", \"centroid_col\"]].values   #第二行的位置\n",
    "        \n",
    "        \n",
    "#         x = label_fir[ind_place_col].numpy().squeeze()\n",
    "#         first_position = df.loc[x, [\"centroid_row\", \"centroid_col\"]].values     #第一行的位置\n",
    "\n",
    "#         distance = ((curr_position - first_position) ** 2).sum(axis=-1)\n",
    "#         nearest_cell = np.argmin(distance, axis=-1)\n",
    "#         node_remain = x[nearest_cell]     #要保留的ID\n",
    "\n",
    "\n",
    "#         delet_id = np.delete(x, np.where(x == node_remain))#需要改数据类型\n",
    "\n",
    "#         for del_id in delet_id:\n",
    "#             # 使用布尔索引删除连接\n",
    "#             delet_mask = np.logical_and(connected_indices[0, :] == del_id, connected_indices[1, :] == label)\n",
    "#             delete_indices = np.where(delet_mask)[0]\n",
    "#             delete_indices_test = np.where(delet_mask)\n",
    "#             connected_indices = np.delete(connected_indices, delete_indices, axis=1)\n",
    "\n",
    "\n",
    "def change_sec_id(self):\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "    # 打印结果\n",
    "    print(\"Connected Indices (After Filtering):\")\n",
    "    print(connected_indices)\n",
    "    print(connected_indices.shape)\n",
    "    print(output_save_pre)\n",
    "    print(output_save_pre.shape)\n",
    "\n",
    "    self.output_pred = output_save_pre\n",
    "    self.edge_index = connected_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 打印结果\n",
    "print(\"Connected Indices (After Filtering):\")\n",
    "print(connected_indices)\n",
    "print(connected_indices.shape)\n",
    "\n",
    "      \n",
    "\n",
    "# next_frame = df.loc[next_frame_ind, [\"centroid_row\", \"centroid_col\"]].values\n",
    "# curr_node = df.loc[i, [\"centroid_row\", \"centroid_col\"]].values\n",
    "\n",
    "# distance = ((next_frame - curr_node) ** 2).sum(axis=-1)\n",
    "# nearest_cell = np.argmin(distance, axis=-1)\n",
    "# # add to the array\n",
    "# next_node_ind = next_frame_ind[nearest_cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0,  ..., 0, 0, 1], dtype=torch.int32)\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 使用断言确保 edge_index 的形状匹配\n",
    "# assert torch.all(edge_index[:, ::2] == edge_index[[1, 0], 1::2]), \"The results don't match!\"\n",
    "\n",
    "# 对 edge_index 进行处理，仅保留一部分\n",
    "edge_index = edge_index[:, ::2]\n",
    "\n",
    "# 从 output_pred 中提取输入和输出的预测结果\n",
    "in_output_pred = output_pred[::2]\n",
    "out_output_pred = output_pred[1::2]\n",
    "\n",
    "# if self.merge_operation == 'OR' or self.merge_operation == 'AND':\n",
    "# 使用 sigmoid 函数对预测结果进行软化\n",
    "in_outputs_soft = torch.sigmoid(in_output_pred)\n",
    "in_outputs_hard = (in_outputs_soft > decision_threshold).int()\n",
    "\n",
    "out_outputs_soft = torch.sigmoid(out_output_pred)\n",
    "out_outputs_hard = (out_outputs_soft > decision_threshold).int()\n",
    "\n",
    "# 根据合并操作进行逻辑运算（OR 或 AND）\n",
    "# final_outputs_hard_or = np.bitwise_or(in_outputs_hard, out_outputs_hard) #if self.merge_operation == 'OR' \\\n",
    "# final_outputs_hard_or_and = np.bitwise_and(in_outputs_hard, out_outputs_hard)\n",
    "print(in_outputs_hard)\n",
    "print(out_outputs_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sunrui/cellwork/track/cell-tracker-gnn-main/logs/runs/2023-05-24/12-53-04/checkpoints\n",
      "/home/sunrui/cellwork/track/cell-tracker-gnn-main/logs/runs/2023-05-24/12-53-04\n",
      "config_path: /home/sunrui/cellwork/track/cell-tracker-gnn-main/logs/runs/2023-05-24/12-53-04/.hydra/config.yaml\n",
      "key: trainer value: {'_target_': 'pytorch_lightning.Trainer', 'gpus': 1, 'min_epochs': 1, 'max_epochs': 500, 'weights_summary': 'top', 'progress_bar_refresh_rate': 100, 'terminate_on_nan': True}\n",
      "key: model value: {'_target_': 'src.models.celltrack_plmodel.CellTrackLitModel', 'one_hot_label': '${one_hot}', 'loss_weights': 65, 'sample': False, 'weight_loss': True, 'directed': True, 'separate_models': '${separate_models}', 'optim_module': {'target': 'Adam', 'kwargs': {'lr': 0.001, 'weight_decay': 1e-05}}, 'lr_sch_module': {'target': None, 'monitor': 'val/acc', 'kwargs': {'verbose': True, 'mode': 'min'}}, 'model_params': {'target': 'CellTrack_Model', 'kwargs': {'hand_NodeEncoder_dic': {'input_dim': 13, 'fc_dims': [64, 16]}, 'learned_NodeEncoder_dic': {'input_dim': 128, 'fc_dims': [64, 16]}, 'intialize_EdgeEncoder_dic': {'input_dim': 239, 'fc_dims': [128, 64]}, 'message_passing': {'target': 'CellTrack_GNN', 'kwargs': {'in_channels': 32, 'hidden_channels': 32, 'in_edge_channels': 64, 'hidden_edge_channels_conv': 16, 'hidden_edge_channels_linear': [128, 64], 'dropout': 0.0, 'num_layers': 6, 'num_nodes_features': 3}}, 'edge_classifier_dic': {'input_dim': 64, 'fc_dims': [128, 32, 1], 'dropout_p': 0.2, 'use_batchnorm': False}}}}\n",
      "key: datamodule value: {'_target_': 'src.datamodules.celltrack_datamodule_mulSeq.CellTrackDataModule', 'data_dir': '${data_dir}', 'batch_size': 8, 'num_workers': 0, 'pin_memory': False, 'train_val_test_split': [80, 20, 0], 'sampler_type': 'from_both', 'dataset_params': {'num_frames': 10, 'type_file': 'csv', 'produce_gt': 'simple', 'main_path': './data/ct_features/SUNR', 'dirs_path': {'train': ['01', '02']}, 'one_hot_label': '${one_hot}', 'exp_name': '2D_SIM', 'overlap': 1, 'jump_frames': 1, 'mul_vals': [2, 2, 2], 'filter_edges': True, 'directed': True, 'edge_feat_embed_dict': {'p': 1, 'use_normalized_x': True, 'normalized_features': False}, 'separate_models': '${separate_models}', 'normalize_all_cols': False, 'same_frame': False, 'next_frame': True, 'self_loop': True, 'normalize': True, 'debug_visualization': False, 'which_preprocess': 'MinMax', 'drop_feat': []}}\n",
      "key: callbacks value: {'model_checkpoint': {'_target_': 'pytorch_lightning.callbacks.ModelCheckpoint', 'monitor': 'val/acc', 'save_top_k': 1, 'save_last': True, 'mode': 'max', 'verbose': False, 'dirpath': 'checkpoints/', 'filename': '{epoch:02d}'}, 'early_stopping': {'_target_': 'pytorch_lightning.callbacks.EarlyStopping', 'monitor': 'val/acc', 'patience': 50, 'mode': 'max', 'min_delta': 0}}\n",
      "key: logger value: {'tensorboard': {'_target_': 'pytorch_lightning.loggers.tensorboard.TensorBoardLogger', 'save_dir': 'tensorboard/', 'name': 'default'}, 'csv': {'_target_': 'pytorch_lightning.loggers.csv_logs.CSVLogger', 'save_dir': '.', 'name': 'csv/'}}\n",
      "key: one_hot value: True\n",
      "key: separate_models value: True\n",
      "key: work_dir value: ${hydra:runtime.cwd}\n",
      "key: data_dir value: ${work_dir}/data/\n",
      "key: debug value: False\n",
      "key: print_config value: True\n",
      "key: disable_warnings value: True\n",
      "..............................................\n",
      "key: _target_ value: src.datamodules.celltrack_datamodule_mulSeq.CellTrackDataModule\n",
      "key: data_dir value: ${data_dir}\n",
      "key: batch_size value: 8\n",
      "key: num_workers value: 0\n",
      "key: pin_memory value: False\n",
      "key: train_val_test_split value: [80, 20, 0]\n",
      "key: sampler_type value: from_both\n",
      "key: dataset_params value: {'num_frames': 10, 'type_file': 'csv', 'produce_gt': 'simple', 'main_path': './data/ct_features/SUNR', 'dirs_path': {'train': ['01', '02']}, 'one_hot_label': '${one_hot}', 'exp_name': '2D_SIM', 'overlap': 1, 'jump_frames': 1, 'mul_vals': [2, 2, 2], 'filter_edges': True, 'directed': True, 'edge_feat_embed_dict': {'p': 1, 'use_normalized_x': True, 'normalized_features': False}, 'separate_models': '${separate_models}', 'normalize_all_cols': False, 'same_frame': False, 'next_frame': True, 'self_loop': True, 'normalize': True, 'debug_visualization': False, 'which_preprocess': 'MinMax', 'drop_feat': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51806/913783244.py:9: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/sunrui/cellwork/track/cell-tracker-gnn-main/logs/runs/2023-05-24/12-53-04/.hydra/config.yaml' mode='r' encoding='UTF-8'>\n",
      "  config = yaml.load(open(config_path),Loader=yaml.Loader)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "CKPT_PATH = model_path\n",
    "path_output = output_csv\n",
    "folder_path = CKPT_PATH\n",
    "for i in range(2):\n",
    "    folder_path = folder_path[:folder_path.rfind('/')]\n",
    "    print(folder_path)\n",
    "config_path = os.path.join(folder_path, '.hydra/config.yaml')\n",
    "print('config_path:', config_path)\n",
    "config = yaml.load(open(config_path),Loader=yaml.Loader)\n",
    "# print(type(config))\n",
    "# print(config.items())\n",
    "for key,value in config.items():\n",
    "    print('key:',key,'value:',value)\n",
    "\n",
    "print('..............................................')\n",
    "\n",
    "\n",
    "data_yaml = config['datamodule']\n",
    "\n",
    "for key,value in data_yaml.items():\n",
    "    print('key:',key,'value:',value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"directed\":        True\n",
      "\"loss_weights\":    65\n",
      "\"lr\":              0.001\n",
      "\"lr_sch_module\":   {'target': None, 'monitor': 'val/acc', 'kwargs': {'verbose': True, 'mode': 'min'}}\n",
      "\"model_params\":    {'target': 'CellTrack_Model', 'kwargs': {'hand_NodeEncoder_dic': {'input_dim': 13, 'fc_dims': [64, 16]}, 'learned_NodeEncoder_dic': {'input_dim': 128, 'fc_dims': [64, 16]}, 'intialize_EdgeEncoder_dic': {'input_dim': 239, 'fc_dims': [128, 64]}, 'message_passing': {'target': 'CellTrack_GNN', 'kwargs': {'in_channels': 32, 'hidden_channels': 32, 'in_edge_channels': 64, 'hidden_edge_channels_conv': 16, 'hidden_edge_channels_linear': [128, 64], 'dropout': 0.0, 'num_layers': 6, 'num_nodes_features': 3}}, 'edge_classifier_dic': {'input_dim': 64, 'fc_dims': [128, 32, 1], 'dropout_p': 0.2, 'use_batchnorm': False}}}\n",
      "\"one_hot_label\":   True\n",
      "\"optim_module\":    {'target': 'Adam', 'kwargs': {'lr': 0.001, 'weight_decay': 1e-05}}\n",
      "\"sample\":          False\n",
      "\"separate_models\": True\n",
      "\"weight_decay\":    0.0005\n",
      "\"weight_loss\":     True\n",
      "<class 'src.models.celltrack_plmodel.CellTrackLitModel'>\n"
     ]
    }
   ],
   "source": [
    "trained_model = CellTrackLitModel.load_from_checkpoint(checkpoint_path=CKPT_PATH)\n",
    "print(trained_model.hparams)\n",
    "trained_model.eval()\n",
    "trained_model.freeze()\n",
    "print(type(trained_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_frames': 'all', 'type_file': 'csv', 'produce_gt': 'simple', 'main_path': '/data/sunrui/celldata/r03c06f04ch1/', 'dirs_path': {'train': ['01', '02'], 'test': ['01_CSV']}, 'one_hot_label': '${one_hot}', 'exp_name': '2D_SIM', 'overlap': 1, 'jump_frames': 1, 'mul_vals': [2, 2, 2], 'filter_edges': True, 'directed': True, 'edge_feat_embed_dict': {'p': 1, 'use_normalized_x': True, 'normalized_features': False}, 'separate_models': '${separate_models}', 'normalize_all_cols': False, 'same_frame': False, 'next_frame': True, 'self_loop': True, 'normalize': True, 'debug_visualization': False, 'which_preprocess': 'MinMax', 'drop_feat': []}\n",
      "key: num_frames value: all\n",
      "key: type_file value: csv\n",
      "key: produce_gt value: simple\n",
      "key: main_path value: /data/sunrui/celldata/r03c06f04ch1/\n",
      "key: dirs_path value: {'train': ['01', '02'], 'test': ['01_CSV']}\n",
      "key: one_hot_label value: ${one_hot}\n",
      "key: exp_name value: 2D_SIM\n",
      "key: overlap value: 1\n",
      "key: jump_frames value: 1\n",
      "key: mul_vals value: [2, 2, 2]\n",
      "key: filter_edges value: True\n",
      "key: directed value: True\n",
      "key: edge_feat_embed_dict value: {'p': 1, 'use_normalized_x': True, 'normalized_features': False}\n",
      "key: separate_models value: ${separate_models}\n",
      "key: normalize_all_cols value: False\n",
      "key: same_frame value: False\n",
      "key: next_frame value: True\n",
      "key: self_loop value: True\n",
      "key: normalize value: True\n",
      "key: debug_visualization value: False\n",
      "key: which_preprocess value: MinMax\n",
      "key: drop_feat value: []\n",
      "Start with /data/sunrui/celldata/r03c06f04ch1/01_CSV/csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunrui/cellwork/track/cell-tracker-gnn-main/src/inference/graph_dataset_inference.py:353: UserWarning: Find the seg label as part of the features and dropped it, please be aware\n",
      "  warnings.warn(\"Find the seg label as part of the features and dropped it, please be aware\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: Index(['frame_num', 'area', 'min_row_bb', 'min_col_bb', 'max_row_bb',\n",
      "       'max_col_bb', 'centroid_row', 'centroid_col', 'major_axis_length',\n",
      "       'minor_axis_length',\n",
      "       ...\n",
      "       'feat_118', 'feat_119', 'feat_120', 'feat_121', 'feat_122', 'feat_123',\n",
      "       'feat_124', 'feat_125', 'feat_126', 'feat_127'],\n",
      "      dtype='object', length=141)\n",
      "Finish frame index 0\n",
      "Finish process /data/sunrui/celldata/r03c06f04ch1/01_CSV/csv (test)\n"
     ]
    }
   ],
   "source": [
    "data_yaml['dataset_params']['num_frames'] = 'all'\n",
    "data_yaml['dataset_params']['main_path'] = path_output\n",
    "\n",
    "second_path = num_seq\n",
    "data_yaml['dataset_params']['dirs_path']['test'] = [second_path + \"_CSV\"]\n",
    "test_yaml = data_yaml['dataset_params']\n",
    "\n",
    "print(test_yaml)\n",
    "for key,value in test_yaml.items():\n",
    "    print('key:',key,'value:',value)\n",
    "dirs_path = test_yaml['dirs_path']\n",
    "data_train: CellTrackDataset = CellTrackDataset(**data_yaml['dataset_params'], split='test')\n",
    "data_list, df_list = data_train.all_data['test']\n",
    "test_data, df_data = data_list[0], df_list[0]\n",
    "\n",
    "x, x2, edge_index, edge_feature = test_data.x, test_data.x_2, test_data.edge_index, test_data.edge_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from collections.abc import Iterable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "dirs_path = test_yaml['dirs_path']\n",
    "main_path = test_yaml['main_path']\n",
    "type_file = test_yaml['type_file']\n",
    "num_frames = test_yaml['num_frames']\n",
    "directed = True\n",
    "same_frame=True\n",
    "next_frame=True\n",
    "self_loop=True\n",
    "filter_edges = False\n",
    "jump_frames = 1\n",
    "overlap = 1\n",
    "mul_vals = [2, 2, 2]\n",
    "def bb_roi(df_data):\n",
    "    print('bb_roi')\n",
    "    cols = ['min_row_bb', 'min_col_bb', 'max_row_bb', 'max_col_bb']\n",
    "\n",
    "    bb_feat = df_data.loc[:, cols]\n",
    "    max_row = np.abs(bb_feat.min_row_bb.values - bb_feat.max_row_bb.values).max()\n",
    "    max_col = np.abs(bb_feat.min_col_bb.values - bb_feat.max_col_bb.values).max()\n",
    "\n",
    "    curr_roi = {'row': max_row * mul_vals[0], 'col': max_col * mul_vals[1]}\n",
    "    return curr_roi\n",
    "\n",
    "\n",
    "def find_roi(files, curr_dir):\n",
    "    print('find_roi')\n",
    "    temp_data = [pd.read_csv(file) for file in files]\n",
    "    df_data = pd.concat(temp_data, axis=0).reset_index(drop=True)\n",
    "    curr_roi = bb_roi(df_data)\n",
    "    return curr_roi\n",
    "\n",
    "\n",
    "def filter_by_roi():\n",
    "    pass\n",
    "\n",
    "def same_next_links(df_data, link_edges):\n",
    "    #doing aggregation of the same frame links + the links between 2 consecutive frames\n",
    "    global same_frame, self_loop, next_frame\n",
    "\n",
    "    same_next_edge_index = []\n",
    "    iter_frames = np.unique(df_data.frame_num.values)\n",
    "    print(iter_frames)\n",
    "    for loop_ind, frame_ind in enumerate(iter_frames[:-1]):\n",
    "\n",
    "        mask_frame = df_data.frame_num.isin([frame_ind])\n",
    "        nodes = df_data.index[mask_frame].values.tolist()\n",
    "        # doing aggregation of the same frame links\n",
    "        if same_frame:\n",
    "            if self_loop:\n",
    "                same_next_edge_index += [list(tup) for tup in itertools.product(nodes, nodes)]\n",
    "            else:\n",
    "                same_next_edge_index += [list(tup) for tup in itertools.product(nodes, nodes) if tup[0] != tup[1]]\n",
    "            \n",
    "            # print(same_next_edge_index)\n",
    "        if next_frame:\n",
    "            if frame_ind != iter_frames[-1]:\n",
    "                # find the places containing the specific frame index\n",
    "                mask_next_frame = df_data.frame_num.isin([iter_frames[loop_ind + 1]])\n",
    "                next_nodes = df_data.index[mask_next_frame].values.tolist()\n",
    "                if filter_edges:\n",
    "                    curr_list = filter_by_roi(df_data.loc[mask_frame, :], df_data.loc[mask_next_frame, :])\n",
    "                    curr_list = list(filter(lambda x: not (x in link_edges), curr_list))\n",
    "                else:\n",
    "                    curr_list =  [list(tup) for tup in itertools.product(nodes, next_nodes)\n",
    "                                    if not (list(tup) in link_edges)]\n",
    "                if not directed:\n",
    "                    # take the opposite direction using [::-1] and merge one-by-one\n",
    "                    # with directed and undirected edges\n",
    "                    curr_list_opposite = [pairs[::-1] for pairs in curr_list]\n",
    "                    curr_list = list(itertools.chain.from_iterable(zip(curr_list, curr_list_opposite)))\n",
    "                same_next_edge_index += curr_list\n",
    "    return same_next_edge_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_graph(curr_dir, mode):\n",
    "    data_list = []\n",
    "    df_list = []\n",
    "    drop_col_list = []\n",
    "    is_first_time = True\n",
    "    global num_frames, jump_frames, type_file, same_frame, next_frame\n",
    "    files = [osp.join(curr_dir, f_name) for f_name in sorted(os.listdir(curr_dir)) if\n",
    "            type_file in f_name]\n",
    "    print(f\"Start with {curr_dir}\")\n",
    "    num_files = len(files)\n",
    "    curr_roi = find_roi(files, curr_dir)\n",
    "    print(curr_roi)\n",
    "    \n",
    "    if num_frames == 'all':\n",
    "        num_frames = num_files\n",
    "    elif isinstance(num_frames, int):\n",
    "        num_frames = num_frames\n",
    "\n",
    "    for ind in range(0, num_files, overlap):\n",
    "        if ind + num_frames > num_files:\n",
    "            break\n",
    "        #read current frame CSVs\n",
    "        temp_data = [pd.read_csv(files[ind_tmp]) for ind_tmp in range(ind, ind + num_frames, jump_frames)]\n",
    "        df_data = pd.concat(temp_data, axis=0).reset_index(drop=True)\n",
    "        print(\"df_data\")\n",
    "        # for key,value in df_data.items():\n",
    "        #     print('key:',key,'value:',value)\n",
    "        #     print('key:',key)\n",
    "        print(df_data)\n",
    "        link_edges = []\n",
    "        if same_frame or next_frame:\n",
    "            same_next_links(df_data, link_edges)\n",
    "            # link_edges += same_next_links(df_data, link_edges)\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'curr_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m frame_ind \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      2\u001b[0m ind \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(curr_dir)\n\u001b[1;32m      4\u001b[0m same_next_edge_index \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m files \u001b[39m=\u001b[39m [osp\u001b[39m.\u001b[39mjoin(curr_dir, f_name) \u001b[39mfor\u001b[39;00m f_name \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39mlistdir(curr_dir)) \u001b[39mif\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         type_file \u001b[39min\u001b[39;00m f_name]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'curr_dir' is not defined"
     ]
    }
   ],
   "source": [
    "frame_ind = 0\n",
    "ind = 0\n",
    "print(curr_dir)\n",
    "same_next_edge_index = []\n",
    "files = [osp.join(curr_dir, f_name) for f_name in sorted(os.listdir(curr_dir)) if\n",
    "        type_file in f_name]\n",
    "temp_data = [pd.read_csv(files[ind_tmp]) for ind_tmp in range(ind, ind + num_frames, jump_frames)]\n",
    "df_data = pd.concat(temp_data, axis=0).reset_index(drop=True)\n",
    "# print(df_data)\n",
    "mask_frame = df_data.frame_num.isin([frame_ind])\n",
    "# print(mask_frame)\n",
    "nodes = df_data.index[mask_frame].values.tolist()\n",
    "same_next_edge_index += [list(tup) for tup in itertools.product(nodes, nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['/data/sunrui/celldata/r03c06f04ch1/01', '/data/sunrui/celldata/r03c06f04ch1/02'], 'test': ['/data/sunrui/celldata/r03c06f04ch1/01_CSV']}\n",
      "['/data/sunrui/celldata/r03c06f04ch1/01_CSV']\n",
      "curr_dir: /data/sunrui/celldata/r03c06f04ch1/01_CSV/csv\n",
      "Start with /data/sunrui/celldata/r03c06f04ch1/01_CSV/csv\n",
      "find_roi\n",
      "bb_roi\n",
      "{'row': 484, 'col': 276}\n",
      "df_data\n",
      "       seg_label  frame_num  area  min_row_bb  min_col_bb  max_row_bb  \\\n",
      "0             13          0  6868         933         220        1023   \n",
      "1             14          0  6855         674         318         774   \n",
      "2             15          0  6568         303         133         400   \n",
      "3             16          0  6489         766         805         872   \n",
      "4             17          0  6009         202          96         293   \n",
      "...          ...        ...   ...         ...         ...         ...   \n",
      "16167         22        328  2341        1030         795        1080   \n",
      "16168         23        328  1918         840         781         907   \n",
      "16169         24        328  1529         683         531         723   \n",
      "16170         25        328  1405         293         741         346   \n",
      "16171         26        328  1037         185          13         225   \n",
      "\n",
      "       max_col_bb  centroid_row  centroid_col  major_axis_length  ...  \\\n",
      "0             320           975           273         102.763495  ...   \n",
      "1             406           722           362         102.618802  ...   \n",
      "2             227           349           180         100.577336  ...   \n",
      "3             887           819           844         111.286256  ...   \n",
      "4             179           247           136          92.142709  ...   \n",
      "...           ...           ...           ...                ...  ...   \n",
      "16167         848          1056           821          57.155569  ...   \n",
      "16168         821           872           802          65.194997  ...   \n",
      "16169         581           704           558          51.199647  ...   \n",
      "16170         772           319           756          56.883003  ...   \n",
      "16171          45           205            29          40.787358  ...   \n",
      "\n",
      "       feat_118  feat_119  feat_120  feat_121  feat_122  feat_123  feat_124  \\\n",
      "0     -0.068724 -0.001491  0.062588  0.041621  0.004609  0.026144  0.113543   \n",
      "1     -0.025499  0.000846  0.090083 -0.091706 -0.002669 -0.056429  0.002700   \n",
      "2      0.015305  0.016789  0.142422 -0.170040  0.027185  0.068836 -0.020006   \n",
      "3      0.021250 -0.021766 -0.069649 -0.230508  0.005433 -0.012273  0.032244   \n",
      "4      0.032306  0.034514  0.123651 -0.154474  0.017248  0.045479  0.099014   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "16167  0.036897  0.058649  0.128542 -0.158333  0.003677  0.053641  0.140292   \n",
      "16168  0.026693  0.060937  0.142629 -0.168661  0.003767  0.059341  0.140497   \n",
      "16169  0.036045  0.061179  0.125745 -0.170306  0.003521  0.056830  0.147532   \n",
      "16170  0.033681  0.058854  0.134773 -0.187179  0.004784  0.067691  0.142710   \n",
      "16171  0.027003  0.064229  0.128817 -0.160316  0.000764  0.052347  0.167809   \n",
      "\n",
      "       feat_125  feat_126  feat_127  \n",
      "0     -0.059798 -0.056886  0.028692  \n",
      "1     -0.050495 -0.060054 -0.008215  \n",
      "2     -0.069881  0.026691  0.018279  \n",
      "3     -0.021707 -0.007533 -0.019140  \n",
      "4     -0.039098 -0.010912  0.023662  \n",
      "...         ...       ...       ...  \n",
      "16167  0.041036 -0.004024  0.056446  \n",
      "16168  0.037341  0.000138  0.047508  \n",
      "16169  0.043149 -0.000226  0.040685  \n",
      "16170  0.032446 -0.004326  0.042593  \n",
      "16171  0.040411 -0.012194  0.035770  \n",
      "\n",
      "[16172 rows x 142 columns]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328]\n",
      "/data/sunrui/celldata/r03c06f04ch1/01_CSV/csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k, v_list in dirs_path.items():\n",
    "    for ind, val in enumerate(v_list):\n",
    "        dirs_path[k][ind] = osp.join(main_path, val)\n",
    "print(dirs_path)\n",
    "split='test'\n",
    "curr_mode = split\n",
    "all_data = {}\n",
    "curr_dir = dirs_path[curr_mode]\n",
    "\n",
    "print(curr_dir)\n",
    "\n",
    "if isinstance(curr_dir, str):\n",
    "    curr_dir = osp.join(curr_dir,type_file)\n",
    "    print(curr_dir)\n",
    "\n",
    "elif isinstance(curr_dir, Iterable):\n",
    "    data_list_t = []\n",
    "    for dir_path in curr_dir:\n",
    "        curr_dir = osp.join(dir_path, type_file)\n",
    "        print('curr_dir:',curr_dir)\n",
    "        create_graph(curr_dir,curr_mode)\n",
    "        # data_list_t += create_graph(curr_dir,curr_mode)\n",
    "\n",
    "print(curr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save path : /data/sunrui/celldata/r03c06f04ch1/01_RES_inference\n",
      "Save inference files: \n",
      " - /data/sunrui/celldata/r03c06f04ch1/01_RES_inference/pytorch_geometric_data.pt \n",
      " - /data/sunrui/celldata/r03c06f04ch1/01_RES_inference/all_data_df.csv \n",
      " - /data/sunrui/celldata/r03c06f04ch1/01_RES_inference/raw_output.pt\n"
     ]
    }
   ],
   "source": [
    "outputs = trained_model((x, x2), edge_index, edge_feature.float())\n",
    "data_path = os.path.join(path_output, second_path) + '_RES_inference'\n",
    "path_output_folder = data_path\n",
    "print(f\"save path : {path_output_folder}\")\n",
    "os.makedirs(path_output_folder, exist_ok=True)\n",
    "file1 = os.path.join(path_output_folder, 'pytorch_geometric_data.pt')\n",
    "file2 = os.path.join(path_output_folder, 'all_data_df.csv')\n",
    "file3 = os.path.join(path_output_folder, 'raw_output.pt')\n",
    "print(f\"Save inference files: \\n - {file1} \\n - {file2} \\n - {file3}\")\n",
    "# df_data.to_csv(file2)\n",
    "# torch.save(test_data, file1)\n",
    "# torch.save(outputs, file3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-tracking-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
