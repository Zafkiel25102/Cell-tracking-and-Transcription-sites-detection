{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'DataParallel' object has no attribute 'image_encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb 单元格 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m sam\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# print(sam.device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# sam_model = sam.module\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m predictor \u001b[39m=\u001b[39m SamPredictor(sam)\u001b[39m#加参数\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m predictor \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDataParallel(predictor)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m predictor\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/predictor.py:31\u001b[0m, in \u001b[0;36mSamPredictor.__init__\u001b[0;34m(self, sam_model)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m sam_model\n\u001b[0;32m---> 31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m ResizeLongestSide(sam_model\u001b[39m.\u001b[39;49mimage_encoder\u001b[39m.\u001b[39mimg_size)\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_image()\n",
      "File \u001b[0;32m~/anaconda3/envs/sam/lib/python3.8/site-packages/torch/nn/modules/module.py:778\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m    777\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m--> 778\u001b[0m \u001b[39mraise\u001b[39;00m ModuleAttributeError(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    779\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'DataParallel' object has no attribute 'image_encoder'"
     ]
    }
   ],
   "source": [
    "#1\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
    "\n",
    "\n",
    "#只需修改路径\n",
    "path= r'/data/sunrui/celldata/20231114_large/data//PRE/test.tif'\n",
    "path_output = r'/data/sunrui/celldata/20231114_large/data//01_GT/SAMSEG/'\n",
    "\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "\n",
    "sam_checkpoint = \"/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "\n",
    "sam = nn.DataParallel(sam)\n",
    "sam.to(device=device)\n",
    "# print(sam.device)\n",
    "\n",
    "# sam_model = sam.module\n",
    "\n",
    "predictor = SamPredictor(sam)#加参数\n",
    "mask_generator = SamAutomaticMaskGenerator(sam,pred_iou_thresh=0.95)#\n",
    "\n",
    "#\n",
    "def prompts_of_disappear_masks(masks_old, masks_new_auto):\n",
    "    prompts = []\n",
    "    masks_new_auto_result = np.zeros((masks_new_auto[0]['segmentation'].shape[0], masks_new_auto[0]['segmentation'].shape[1]))       #1000*1000\n",
    "    for mask in masks_new_auto:\n",
    "        masks_new_auto_result = np.logical_or(masks_new_auto_result, mask['segmentation'])\n",
    "    for mask in masks_old:\n",
    "        mask_use = np.array(mask['bbox'])\n",
    "        box=mask_use.astype(int)\n",
    "        if not masks_new_auto_result[box[1]+box[3]//2, box[0]+box[2]//2]:\n",
    "            prompts.append([box[0]+box[2]//2, box[1]+box[3]//2])\n",
    "    return prompts\n",
    "\n",
    "def get_masks_new(image, prompts, masks_new_auto):\n",
    "    #masks_new = masks_new_auto.copy()\n",
    "    predictor.set_image(image)\n",
    "    for prompt in prompts:\n",
    "        input_point = np.array([prompt])\n",
    "        input_label = np.array([1])\n",
    "        #mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n",
    "\n",
    "        mask, _, _ = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        #mask_input=mask_input[None, :, :],\n",
    "        multimask_output=False,\n",
    "        )\n",
    "        mask=mask.squeeze()\n",
    "        xyxy = np.where(mask)\n",
    "        # x1,y1,x2,y2 = xyxy[0].min(), xyxy[1].min(), xyxy[0].max(), xyxy[1].max()          #这里也是矩阵，到下面应该是图像中的坐标了\n",
    "        x1,y1,x2,y2 = xyxy[1].min(), xyxy[0].min(), xyxy[1].max(), xyxy[0].max()\n",
    "\n",
    "        bbox= [x1,y1,x2-x1,y2-y1]\n",
    "        area= len(xyxy[0])\n",
    "\n",
    "        masks_new_auto.append(\n",
    "            {\n",
    "                'segmentation':mask,\n",
    "                'bbox':bbox,\n",
    "                'area':area,\n",
    "                'prompt':prompt\n",
    "            }\n",
    "        )\n",
    "    return masks_new_auto\n",
    "\n",
    "def save_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    # 按面积大小对注释进行排序\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    img_shape = (sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1],1)        #1000*1000*1\n",
    "    \n",
    "    # 创建一个空白图像\n",
    "    # img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img = np.zeros(img_shape, dtype=np.uint16)\n",
    "    img[:,:,0] = 0\n",
    "    for i, ann in enumerate(sorted_anns):\n",
    "        mask_thresholdU = 12000\n",
    "        mask_thresholdD = 750\n",
    "        mask_area = ann['area']\n",
    "        m = ann['segmentation']         #python中1和True，0和False是一回事儿，m就是二值灰度图,假的！！！不是一回事儿\n",
    "        # “is”和“==”的含义不同，“1”和“True”虽然数值相同，但是id不同。\n",
    "\n",
    "        # #后处理\n",
    "        # structuring_element = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        # # 进行腐蚀操作\n",
    "        # eroded_image_0 = cv2.erode(m.astype('uint8'), structuring_element)              #得astype\n",
    "        # eroded_image_1 = cv2.erode(eroded_image_0, structuring_element)\n",
    "        # # 进行膨胀操作\n",
    "        # dilated_image_0 = cv2.dilate(eroded_image_1, structuring_element)\n",
    "        # dilated_image_1 = cv2.dilate(dilated_image_0, structuring_element).astype('bool')# 这也好像得astype\n",
    "\n",
    "        if(mask_area < mask_thresholdU and mask_area > mask_thresholdD):\n",
    "            # gray_value = 2*i + 1\n",
    "            gray_value = i + 1\n",
    "            # color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "            img[m] = gray_value                               \n",
    "            #当前维度的值相等。\n",
    "            # 当前维度的值有一个是 1。触发广播\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 14.28 GiB (GPU 0; 10.75 GiB total capacity; 3.84 GiB already allocated; 5.17 GiB free; 4.61 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb 单元格 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(img,cv2\u001b[39m.\u001b[39mCOLOR_GRAY2RGB)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m masks_old \u001b[39m=\u001b[39m mask_generator\u001b[39m.\u001b[39;49mgenerate(img)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m masks_new \u001b[39m=\u001b[39m masks_old\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/sam_pipeline.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m output_result \u001b[39m=\u001b[39m save_anns(masks_new)\n",
      "File \u001b[0;32m~/anaconda3/envs/sam/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/automatic_mask_generator.py:180\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator.generate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mGenerates masks for the given image.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m# Generate masks\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m mask_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_masks(image)\n\u001b[1;32m    182\u001b[0m \u001b[39m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_mask_region_area \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/automatic_mask_generator.py:223\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    221\u001b[0m data \u001b[39m=\u001b[39m MaskData()\n\u001b[1;32m    222\u001b[0m \u001b[39mfor\u001b[39;00m crop_box, layer_idx \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[0;32m--> 223\u001b[0m     crop_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_crop(image, crop_box, layer_idx, orig_size)\n\u001b[1;32m    224\u001b[0m     data\u001b[39m.\u001b[39mcat(crop_data)\n\u001b[1;32m    226\u001b[0m \u001b[39m# Remove duplicate masks between crops\u001b[39;00m\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/automatic_mask_generator.py:262\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    260\u001b[0m data \u001b[39m=\u001b[39m MaskData()\n\u001b[1;32m    261\u001b[0m \u001b[39mfor\u001b[39;00m (points,) \u001b[39min\u001b[39;00m batch_iterator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoints_per_batch, points_for_image):\n\u001b[0;32m--> 262\u001b[0m     batch_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_batch(points, cropped_im_size, crop_box, orig_size)\n\u001b[1;32m    263\u001b[0m     data\u001b[39m.\u001b[39mcat(batch_data)\n\u001b[1;32m    264\u001b[0m     \u001b[39mdel\u001b[39;00m batch_data\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/automatic_mask_generator.py:296\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_batch\u001b[0;34m(self, points, im_size, crop_box, orig_size)\u001b[0m\n\u001b[1;32m    294\u001b[0m in_points \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(transformed_points, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    295\u001b[0m in_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(in_points\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint, device\u001b[39m=\u001b[39min_points\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 296\u001b[0m masks, iou_preds, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor\u001b[39m.\u001b[39;49mpredict_torch(\n\u001b[1;32m    297\u001b[0m     in_points[:, \u001b[39mNone\u001b[39;49;00m, :],\n\u001b[1;32m    298\u001b[0m     in_labels[:, \u001b[39mNone\u001b[39;49;00m],\n\u001b[1;32m    299\u001b[0m     multimask_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    300\u001b[0m     return_logits\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    303\u001b[0m \u001b[39m# Serialize predictions and store in MaskData\u001b[39;00m\n\u001b[1;32m    304\u001b[0m data \u001b[39m=\u001b[39m MaskData(\n\u001b[1;32m    305\u001b[0m     masks\u001b[39m=\u001b[39mmasks\u001b[39m.\u001b[39mflatten(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[1;32m    306\u001b[0m     iou_preds\u001b[39m=\u001b[39miou_preds\u001b[39m.\u001b[39mflatten(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[1;32m    307\u001b[0m     points\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mas_tensor(points\u001b[39m.\u001b[39mrepeat(masks\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)),\n\u001b[1;32m    308\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/sam/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/predictor.py:238\u001b[0m, in \u001b[0;36mSamPredictor.predict_torch\u001b[0;34m(self, point_coords, point_labels, boxes, mask_input, multimask_output, return_logits)\u001b[0m\n\u001b[1;32m    229\u001b[0m low_res_masks, iou_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmask_decoder(\n\u001b[1;32m    230\u001b[0m     image_embeddings\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures,\n\u001b[1;32m    231\u001b[0m     image_pe\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mprompt_encoder\u001b[39m.\u001b[39mget_dense_pe(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     multimask_output\u001b[39m=\u001b[39mmultimask_output,\n\u001b[1;32m    235\u001b[0m )\n\u001b[1;32m    237\u001b[0m \u001b[39m# Upscale the masks to the original image resolution\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpostprocess_masks(low_res_masks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moriginal_size)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_logits:\n\u001b[1;32m    241\u001b[0m     masks \u001b[39m=\u001b[39m masks \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmask_threshold\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/modeling/sam.py:161\u001b[0m, in \u001b[0;36mSam.postprocess_masks\u001b[0;34m(self, masks, input_size, original_size)\u001b[0m\n\u001b[1;32m    154\u001b[0m masks \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(\n\u001b[1;32m    155\u001b[0m     masks,\n\u001b[1;32m    156\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_encoder\u001b[39m.\u001b[39mimg_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_encoder\u001b[39m.\u001b[39mimg_size),\n\u001b[1;32m    157\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    158\u001b[0m     align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    160\u001b[0m masks \u001b[39m=\u001b[39m masks[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, : input_size[\u001b[39m0\u001b[39m], : input_size[\u001b[39m1\u001b[39m]]\n\u001b[0;32m--> 161\u001b[0m masks \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49minterpolate(masks, original_size, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbilinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m masks\n",
      "File \u001b[0;32m~/anaconda3/envs/sam/lib/python3.8/site-packages/torch/nn/functional.py:3151\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor)\u001b[0m\n\u001b[1;32m   3149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   3150\u001b[0m     \u001b[39massert\u001b[39;00m align_corners \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 3151\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mupsample_bilinear2d(\u001b[39minput\u001b[39;49m, output_size, align_corners, scale_factors)\n\u001b[1;32m   3152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrilinear\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   3153\u001b[0m     \u001b[39massert\u001b[39;00m align_corners \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 14.28 GiB (GPU 0; 10.75 GiB total capacity; 3.84 GiB already allocated; 5.17 GiB free; 4.61 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import skimage.io as skio\n",
    "from tifffile import imread, imwrite\n",
    "\n",
    "\n",
    "# import czifile\n",
    "# img = czifile.imread(root_path +'/01/'+ file_name)\n",
    "\n",
    "img_raw = skio.imread(path,plugin=\"tifffile\")\n",
    "\n",
    "img=img_raw[0]\n",
    "index=0\n",
    "img = img.squeeze()\n",
    "img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
    "masks_old = mask_generator.generate(img)\n",
    "masks_new = masks_old\n",
    "output_result = save_anns(masks_new)\n",
    "# 图片保存的文件夹路径 + 名字\n",
    "image_path = os.path.join(path_output, f'man_seg{index:03d}.tif')\n",
    "imwrite(image_path, output_result)\n",
    "print(f'save successful man_seg{index:03d}.tif')\n",
    "# 在训练迭代中释放不需要的内存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "prompts=[]\n",
    "for img in img_raw[1:,:, :, : ]:\n",
    "    index+=1\n",
    "    img = img.squeeze()\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
    "    masks_new_auto = mask_generator.generate(img)\n",
    "\n",
    "    prompts = prompts_of_disappear_masks(masks_old, masks_new_auto)\n",
    "    masks_new=get_masks_new(img, prompts, masks_new_auto)\n",
    "    masks_old=masks_new\n",
    "    \n",
    "    output_result = save_anns(masks_new)\n",
    "    # 图片保存的文件夹路径 + 名字\n",
    "    image_path = os.path.join(path_output, f'man_seg{index:03d}.tif')\n",
    "    imwrite(image_path, output_result)\n",
    "    print(f'save successful man_seg{index:03d}.tif')\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=np.array([[[1,2],[3,4]]])\n",
    "# b=np.array([[[True,False],[False,True]]])\n",
    "# a[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=np.ones([2,2,1])\n",
    "# b=np.array([[True,False],\n",
    "#             [False,True]])\n",
    "# a[b]=2\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:5: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/tmp/ipykernel_21604/3675708850.py:5: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  1 is 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7618240"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b=np.array([[[True],[False]],\n",
    "#             [[False],[True]]])\n",
    "# a[b]=3\n",
    "# a\n",
    "1 is 1\n",
    "id(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
