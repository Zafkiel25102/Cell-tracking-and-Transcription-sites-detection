{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r'/data/sunrui/celldata/20230724_HBEC_Yoko_Lib5_G+R-_DL/F0002/'\n",
    "file_name = r'F0002.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    # 按面积大小对注释进行排序\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    img_shape = (sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1],1)\n",
    "\n",
    "\n",
    "    \n",
    "    # 创建一个空白图像\n",
    "    # img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img = np.zeros(img_shape, dtype=np.uint8)\n",
    "    img[:,:,0] = 0\n",
    "    for i, ann in enumerate(sorted_anns):\n",
    "        mask_thresholdU = 12000\n",
    "        mask_thresholdD = 1000\n",
    "        mask_area = ann['area']\n",
    "        m = ann['segmentation']\n",
    "        if(mask_area < mask_thresholdU and mask_area > mask_thresholdD):\n",
    "            # gray_value = 2*i + 1\n",
    "            gray_value = i + 1\n",
    "            # color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "            img[m] = gray_value\n",
    "        \n",
    "        \n",
    "        # print(img.shape)\n",
    "    img = img\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sunrui/cellwork/sam/segment-anything/notebooks/CellSeg copy.ipynb 单元格 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/CellSeg%20copy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvit_h\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/CellSeg%20copy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/CellSeg%20copy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m sam \u001b[39m=\u001b[39m sam_model_registry[model_type](checkpoint\u001b[39m=\u001b[39;49msam_checkpoint)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/CellSeg%20copy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m sam\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/CellSeg%20copy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m mask_generator \u001b[39m=\u001b[39m SamAutomaticMaskGenerator(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/CellSeg%20copy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     sam,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/CellSeg%20copy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     pred_iou_thresh \u001b[39m=\u001b[39m \u001b[39m0.95\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/CellSeg%20copy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     crop_overlap_ratio\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/CellSeg%20copy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     min_mask_region_area \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.18.32.110/home/sunrui/cellwork/sam/segment-anything/notebooks/CellSeg%20copy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/build_sam.py:15\u001b[0m, in \u001b[0;36mbuild_sam_vit_h\u001b[0;34m(checkpoint)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_sam_vit_h\u001b[39m(checkpoint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m _build_sam(\n\u001b[1;32m     16\u001b[0m         encoder_embed_dim\u001b[39m=\u001b[39;49m\u001b[39m1280\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m         encoder_depth\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m         encoder_num_heads\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m         encoder_global_attn_indexes\u001b[39m=\u001b[39;49m[\u001b[39m7\u001b[39;49m, \u001b[39m15\u001b[39;49m, \u001b[39m23\u001b[39;49m, \u001b[39m31\u001b[39;49m],\n\u001b[1;32m     20\u001b[0m         checkpoint\u001b[39m=\u001b[39;49mcheckpoint,\n\u001b[1;32m     21\u001b[0m     )\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/build_sam.py:67\u001b[0m, in \u001b[0;36m_build_sam\u001b[0;34m(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, checkpoint)\u001b[0m\n\u001b[1;32m     64\u001b[0m vit_patch_size \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m     65\u001b[0m image_embedding_size \u001b[39m=\u001b[39m image_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m vit_patch_size\n\u001b[1;32m     66\u001b[0m sam \u001b[39m=\u001b[39m Sam(\n\u001b[0;32m---> 67\u001b[0m     image_encoder\u001b[39m=\u001b[39mImageEncoderViT(\n\u001b[1;32m     68\u001b[0m         depth\u001b[39m=\u001b[39;49mencoder_depth,\n\u001b[1;32m     69\u001b[0m         embed_dim\u001b[39m=\u001b[39;49mencoder_embed_dim,\n\u001b[1;32m     70\u001b[0m         img_size\u001b[39m=\u001b[39;49mimage_size,\n\u001b[1;32m     71\u001b[0m         mlp_ratio\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     72\u001b[0m         norm_layer\u001b[39m=\u001b[39;49mpartial(torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mLayerNorm, eps\u001b[39m=\u001b[39;49m\u001b[39m1e-6\u001b[39;49m),\n\u001b[1;32m     73\u001b[0m         num_heads\u001b[39m=\u001b[39;49mencoder_num_heads,\n\u001b[1;32m     74\u001b[0m         patch_size\u001b[39m=\u001b[39;49mvit_patch_size,\n\u001b[1;32m     75\u001b[0m         qkv_bias\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m         use_rel_pos\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     77\u001b[0m         global_attn_indexes\u001b[39m=\u001b[39;49mencoder_global_attn_indexes,\n\u001b[1;32m     78\u001b[0m         window_size\u001b[39m=\u001b[39;49m\u001b[39m14\u001b[39;49m,\n\u001b[1;32m     79\u001b[0m         out_chans\u001b[39m=\u001b[39;49mprompt_embed_dim,\n\u001b[1;32m     80\u001b[0m     ),\n\u001b[1;32m     81\u001b[0m     prompt_encoder\u001b[39m=\u001b[39mPromptEncoder(\n\u001b[1;32m     82\u001b[0m         embed_dim\u001b[39m=\u001b[39mprompt_embed_dim,\n\u001b[1;32m     83\u001b[0m         image_embedding_size\u001b[39m=\u001b[39m(image_embedding_size, image_embedding_size),\n\u001b[1;32m     84\u001b[0m         input_image_size\u001b[39m=\u001b[39m(image_size, image_size),\n\u001b[1;32m     85\u001b[0m         mask_in_chans\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[1;32m     86\u001b[0m     ),\n\u001b[1;32m     87\u001b[0m     mask_decoder\u001b[39m=\u001b[39mMaskDecoder(\n\u001b[1;32m     88\u001b[0m         num_multimask_outputs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m     89\u001b[0m         transformer\u001b[39m=\u001b[39mTwoWayTransformer(\n\u001b[1;32m     90\u001b[0m             depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     91\u001b[0m             embedding_dim\u001b[39m=\u001b[39mprompt_embed_dim,\n\u001b[1;32m     92\u001b[0m             mlp_dim\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m,\n\u001b[1;32m     93\u001b[0m             num_heads\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,\n\u001b[1;32m     94\u001b[0m         ),\n\u001b[1;32m     95\u001b[0m         transformer_dim\u001b[39m=\u001b[39mprompt_embed_dim,\n\u001b[1;32m     96\u001b[0m         iou_head_depth\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m     97\u001b[0m         iou_head_hidden_dim\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,\n\u001b[1;32m     98\u001b[0m     ),\n\u001b[1;32m     99\u001b[0m     pixel_mean\u001b[39m=\u001b[39m[\u001b[39m123.675\u001b[39m, \u001b[39m116.28\u001b[39m, \u001b[39m103.53\u001b[39m],\n\u001b[1;32m    100\u001b[0m     pixel_std\u001b[39m=\u001b[39m[\u001b[39m58.395\u001b[39m, \u001b[39m57.12\u001b[39m, \u001b[39m57.375\u001b[39m],\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    102\u001b[0m sam\u001b[39m.\u001b[39meval()\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m checkpoint \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/modeling/image_encoder.py:74\u001b[0m, in \u001b[0;36mImageEncoderViT.__init__\u001b[0;34m(self, img_size, patch_size, in_chans, embed_dim, depth, num_heads, mlp_ratio, out_chans, qkv_bias, norm_layer, act_layer, use_abs_pos, use_rel_pos, rel_pos_zero_init, window_size, global_attn_indexes)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList()\n\u001b[1;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(depth):\n\u001b[0;32m---> 74\u001b[0m     block \u001b[39m=\u001b[39m Block(\n\u001b[1;32m     75\u001b[0m         dim\u001b[39m=\u001b[39;49membed_dim,\n\u001b[1;32m     76\u001b[0m         num_heads\u001b[39m=\u001b[39;49mnum_heads,\n\u001b[1;32m     77\u001b[0m         mlp_ratio\u001b[39m=\u001b[39;49mmlp_ratio,\n\u001b[1;32m     78\u001b[0m         qkv_bias\u001b[39m=\u001b[39;49mqkv_bias,\n\u001b[1;32m     79\u001b[0m         norm_layer\u001b[39m=\u001b[39;49mnorm_layer,\n\u001b[1;32m     80\u001b[0m         act_layer\u001b[39m=\u001b[39;49mact_layer,\n\u001b[1;32m     81\u001b[0m         use_rel_pos\u001b[39m=\u001b[39;49muse_rel_pos,\n\u001b[1;32m     82\u001b[0m         rel_pos_zero_init\u001b[39m=\u001b[39;49mrel_pos_zero_init,\n\u001b[1;32m     83\u001b[0m         window_size\u001b[39m=\u001b[39;49mwindow_size \u001b[39mif\u001b[39;49;00m i \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m global_attn_indexes \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m     84\u001b[0m         input_size\u001b[39m=\u001b[39;49m(img_size \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m patch_size, img_size \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m patch_size),\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\u001b[39m.\u001b[39mappend(block)\n\u001b[1;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneck \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     89\u001b[0m     nn\u001b[39m.\u001b[39mConv2d(\n\u001b[1;32m     90\u001b[0m         embed_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m     LayerNorm2d(out_chans),\n\u001b[1;32m    104\u001b[0m )\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/modeling/image_encoder.py:152\u001b[0m, in \u001b[0;36mBlock.__init__\u001b[0;34m(self, dim, num_heads, mlp_ratio, qkv_bias, norm_layer, act_layer, use_rel_pos, rel_pos_zero_init, window_size, input_size)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1 \u001b[39m=\u001b[39m norm_layer(dim)\n\u001b[0;32m--> 152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn \u001b[39m=\u001b[39m Attention(\n\u001b[1;32m    153\u001b[0m     dim,\n\u001b[1;32m    154\u001b[0m     num_heads\u001b[39m=\u001b[39;49mnum_heads,\n\u001b[1;32m    155\u001b[0m     qkv_bias\u001b[39m=\u001b[39;49mqkv_bias,\n\u001b[1;32m    156\u001b[0m     use_rel_pos\u001b[39m=\u001b[39;49muse_rel_pos,\n\u001b[1;32m    157\u001b[0m     rel_pos_zero_init\u001b[39m=\u001b[39;49mrel_pos_zero_init,\n\u001b[1;32m    158\u001b[0m     input_size\u001b[39m=\u001b[39;49minput_size \u001b[39mif\u001b[39;49;00m window_size \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m (window_size, window_size),\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2 \u001b[39m=\u001b[39m norm_layer(dim)\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m MLPBlock(embedding_dim\u001b[39m=\u001b[39mdim, mlp_dim\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(dim \u001b[39m*\u001b[39m mlp_ratio), act\u001b[39m=\u001b[39mact_layer)\n",
      "File \u001b[0;32m~/cellwork/sam/segment-anything/segment_anything/modeling/image_encoder.py:213\u001b[0m, in \u001b[0;36mAttention.__init__\u001b[0;34m(self, dim, num_heads, qkv_bias, use_rel_pos, rel_pos_zero_init, input_size)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale \u001b[39m=\u001b[39m head_dim\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqkv \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(dim, dim \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m, bias\u001b[39m=\u001b[39mqkv_bias)\n\u001b[0;32m--> 213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(dim, dim)\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_rel_pos \u001b[39m=\u001b[39m use_rel_pos\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_rel_pos:\n",
      "File \u001b[0;32m~/anaconda3/envs/sam/lib/python3.8/site-packages/torch/nn/modules/linear.py:83\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m---> 83\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m~/anaconda3/envs/sam/lib/python3.8/site-packages/torch/nn/modules/linear.py:86\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/anaconda3/envs/sam/lib/python3.8/site-packages/torch/nn/init.py:384\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    382\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 384\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    sam,\n",
    "    pred_iou_thresh = 0.95,\n",
    "    crop_overlap_ratio=0,\n",
    "    min_mask_region_area = 1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(378, 1000, 1000, 1)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "# image = cv2.imread('images/dog.jpg')\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "import skimage.io as skio\n",
    "from tifffile import imread, imwrite\n",
    "\n",
    "# #读取一系列图像\n",
    "# raw_path = root_path#原始图像路径\n",
    "# imgfiles = [os.path.join(raw_path, f) for f in os.listdir(raw_path) if f.endswith('.tif') or f.endswith('.tiff')]\n",
    "# imgfiles.sort()\n",
    "\n",
    "# img_raw = []\n",
    "# print(len(imgfiles))\n",
    "# for i in range(len(imgfiles)):\n",
    "#     img_raw.append(skio.imread(imgfiles[i]).astype(np.uint16))\n",
    "# print(len(img_raw))\n",
    "# img_raw = np.array(img_raw)\n",
    "# print(img_raw.shape)\n",
    "# print(img_raw.shape[0])\n",
    "\n",
    "\n",
    "img_raw = skio.imread(root_path + r'/PRE/test.tif',plugin=\"tifffile\")\n",
    "# img_raw = skio.imread(root_path + r'/PRE/test_seg.tif',plugin=\"tifffile\")\n",
    "print(img_raw.shape)\n",
    "# img_raw = np.expand_dims(img_raw,axis=3)\n",
    "# x = np.expand_dims(x,axis=3)\n",
    "print(img_raw.dtype)\n",
    "\n",
    "# #uint16转uint8\n",
    "# max_val = np.max(img_raw)\n",
    "# min_val = np.min(img_raw)\n",
    "\n",
    "# # 归一化处理\n",
    "# arr_normalized = (img_raw - min_val) * 255 / (max_val - min_val)\n",
    "\n",
    "# # 将归一化后的数组转换为 uint8 类型\n",
    "# arr_uint8 = arr_normalized.astype(np.uint8)\n",
    "\n",
    "img = img_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_folder =root_path + r'/01_GT/SAMSEG/'\n",
    "if not os.path.exists(image_folder):\n",
    "    os.makedirs(image_folder)\n",
    "\n",
    "for i in range(img.shape[0]):\n",
    "# for i in range(1):\n",
    "    image = img[i]\n",
    "    #uint16转换为uint8\n",
    "    # uint16_img = image\n",
    "    # uint16_img -= image.min()\n",
    "    # uint16_img = uint16_img / (uint16_img.max()-uint16_img.min())\n",
    "    # uint16_img *= 255\n",
    "    # uint8_img = np.uint8(uint16_img)\n",
    "\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # image = image.astype('uint8')\n",
    "    # image = cv2.imread(r'/data/sunrui/celldata/20230414_hesc_Mixl1_yy/r03c03f01/01/t000.tif')\n",
    "    # image = imread(r'/data/sunrui/celldata/20230414_hesc_Mixl1_yy/r03c03f01/01/t000.tif')\n",
    "\n",
    "    # print(image.dtype)\n",
    "    # print(image.shape)\n",
    "    # print('-------------------------------------------------------')\n",
    "\n",
    "    #生成masks\n",
    "    masks = mask_generator.generate(image)\n",
    "    mask_result = show_anns(masks)\n",
    "    # print(mask_result.shape)\n",
    "\n",
    "    # 图片保存的文件夹路径\n",
    "    \n",
    "    image_path = os.path.join(image_folder, f'man_seg{i:03d}.tif')\n",
    "    imwrite(image_path,mask_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000, 1)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(mask_result.shape)\n",
    "print(mask_result.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
